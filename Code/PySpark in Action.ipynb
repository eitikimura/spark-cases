{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Devfest 2017\n",
    "# Apache Spark: Casos de Uso e Escalabilidade\n",
    "\n",
    "## flavio.clesio@movile.com / eiti.kimura@movile.com\n",
    "\n",
    "Este notebook mostra um pequeno exemplo de como usar o PySpark para tarefas de Machine Learning dentro do Spark MLLLib. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Boiler Plate code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "SPARK_PATH = \"/Users/flavio.clesio/Documents/spark-2.1.0\" \n",
    "\n",
    "os.environ['SPARK_HOME'] = SPARK_PATH\n",
    "os.environ['HADOOP_HOME'] = SPARK_PATH\n",
    "\n",
    "sys.path.append(SPARK_PATH + \"/bin\")\n",
    "sys.path.append(SPARK_PATH + \"/python\")\n",
    "sys.path.append(SPARK_PATH + \"/python/pyspark/\")\n",
    "sys.path.append(SPARK_PATH + \"/python/lib\")\n",
    "sys.path.append(SPARK_PATH + \"/python/lib/pyspark.zip\")\n",
    "sys.path.append(SPARK_PATH + \"/python/lib/py4j-0.10.4-src.zip\")\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext(\"local\", \"test\")\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "data = spark.read.format(\"libsvm\").load(SPARK_PATH + \"/data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "data.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Feature Extraction\n",
    "\n",
    "Vamos usar aqui duas transformações do Spark MLLib que são o `StringIndexer` que vai incluir todos os labels em um único índice; e o `VectorIndexer` automaticamente vai indexar as features. O parâmetro `maxCategories` estará em `4` para que as features que tenham mais de `4` valores distintos sejam tratadas pelo modelo como contínuas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n",
    "\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos dividir a nossa base de dados em 70% para treino e 30% para testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Treinamento do modelo \n",
    "\n",
    "Primeiramente vamos chamar a classe `DecisionTreeClassifier` e vamos passar os objetos `StringIndexer` e `VectorIndexer` que construímos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui vamos usar um recurso do Spark MLLib que é o Pipeline.  \n",
    "\n",
    "O conceito do pipeline é colocar em uma sequência encadeada inúmeros algoritmos para o processamento dos dados até mesmo à parte de aprendizado. Cada passo dentro do pipeline é chamado de `PipelineStages` em que podem ser feitas transformações e o uso de estimadores de forma encadeada.  \n",
    "\n",
    "Isso ajuda na simplificação do código, leitura, e principalmente debug. \n",
    "\n",
    "No nosso pipeline vamos encadear o `StringIndexer`, o `VectorIndexer` e a nossa árvore de decisão que está no objeto `dt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste fase vamos realizar o treinamento do modelo chamando o método `.fit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos realizar algumas predições usando a base de testes; e vamos mostrar 5 registros realizando a comparação entre o que foi previsto pelo modelo (_i.e._ a coluna `prediction`) e o que está na base de testes (_i.e._ a coluna `indexedLabel`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+\n",
      "|prediction|indexedLabel|            features|\n",
      "+----------+------------+--------------------+\n",
      "|       1.0|         1.0|(692,[123,124,125...|\n",
      "|       1.0|         1.0|(692,[124,125,126...|\n",
      "|       1.0|         1.0|(692,[124,125,126...|\n",
      "|       1.0|         1.0|(692,[126,127,128...|\n",
      "|       1.0|         1.0|(692,[126,127,128...|\n",
      "+----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(testData)\n",
    "\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Gráfico da nossa árvore de decisão\n",
    "\n",
    "Neste step, vamos ver como ficou a estrutura de decisão da nossa árvore gerada pelo algortimo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_42ceb20c9dc1d26ae66a) of depth 2 with 5 nodes\n",
      "  If (feature 406 <= 20.0)\n",
      "   If (feature 99 in {2.0})\n",
      "    Predict: 0.0\n",
      "   Else (feature 99 not in {2.0})\n",
      "    Predict: 1.0\n",
      "  Else (feature 406 > 20.0)\n",
      "   Predict: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.stages[2].toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Serialização do modelo\n",
    "\n",
    "Todo modelo do MLLib pode ser serializado, _i.e._ pode ser salvo em um arquivo `.parquet` ou `.json` para posterior uso.\n",
    "\n",
    "Vamos salvar o nosso modelo, e realizar a carga novamente e ver a estrutura do modelo para checar se trata do mesmo modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.write().overwrite().save(SPARK_PATH + \"/data/mllib/tmp/myDecisionTreeClassificationModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sameModel = model.load(SPARK_PATH + \"/data/mllib/tmp/myDecisionTreeClassificationModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_42ceb20c9dc1d26ae66a) of depth 2 with 5 nodes\n",
      "  If (feature 406 <= 20.0)\n",
      "   If (feature 99 in {2.0})\n",
      "    Predict: 0.0\n",
      "   Else (feature 99 not in {2.0})\n",
      "    Predict: 1.0\n",
      "  Else (feature 406 > 20.0)\n",
      "   Predict: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sameModel.stages[2].toDebugString)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
